{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b2d052",
   "metadata": {},
   "source": [
    "# Création d'une segmentation client pour Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e147f4",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e38f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768d344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9699d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503f7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7daef037",
   "metadata": {},
   "source": [
    "Problématique : identifier différents comportements des clients afin de pouvoir faire du ciblage marketing et augmenter ainsi la rentabilité de l'entreprise\n",
    "\n",
    "Nous disposons de plusieurs informations anonymisées de la société Olist sur : \n",
    "- les commandes\n",
    "- les clients\n",
    "- les produits\n",
    "- les catégories des produits\n",
    "- des informations sur le paiment des commandes\n",
    "- les vendeurs.\n",
    "\n",
    "Il est important de noter qu'une commande peut contenir des produits de différents vendeurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97df084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notre package de fonctionnalités\n",
    "from Package import Scripts_Analyse01 as pk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from yellowbrick.features import ParallelCoordinates\n",
    "from plotly.graph_objects import Layout\n",
    "import jenkspy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n",
    "from sklearn import preprocessing\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.features import PCA as PCA_yellow\n",
    "from yellowbrick.style import set_palette\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "#Import knearest neighbors Classifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,mutual_info_classif\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    " \n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac185ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "#####encoder les colonnes catégorielles\n",
    "def one_hot_encoder(df, exclude=[], nan_as_category = True):\n",
    "    cols=filter(lambda x: (\"_id\" not in (x)) and x not in (exclude), df.columns)\n",
    "    original_columns = list(cols)\n",
    "    categorical_columns = [col for col in original_columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "def agg_by_group(df_test2, tab, cols, agg_name, col_index, col_by_cat):\n",
    "    for col in cols:\n",
    "        df_calcul=df_test2.pivot_table(values=col, index=col_index, columns=col_by_cat, aggfunc=agg_name).reset_index()\n",
    "        lt_col=list(filter(lambda x: x not in (col_index), df_calcul.columns))\n",
    "        lt_col=[e + \"_\"+str(col) for e in lt_col]\n",
    "        lt_col.insert(0,col_index)\n",
    "        df_calcul.columns=pd.Index([e for e in lt_col])\n",
    "        df_calcul=df_calcul.fillna(0)\n",
    "        tab=tab.merge(df_calcul, how='left', on=col_index)\n",
    "        #display(tab)\n",
    "    return tab\n",
    "\n",
    "def difference_dates(date2, date1, new_column, df, var_i=0):    \n",
    "    df2=df.copy()\n",
    "    if var_i==0:\n",
    "        df2[new_column] = pd.to_numeric((pd.to_datetime(df2[date2]) - pd.to_datetime(df2[date1])).dt.days)\n",
    "    if var_i==1:\n",
    "        df2[new_column] = pd.to_numeric((pd.to_datetime(date2) - pd.to_datetime(df2[date1])).dt.days)\n",
    "    return df2\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def orders(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    ####lecture des fichiers application_train et application test\n",
    "    df = pd.read_csv('./input/olist_orders_dataset.csv', nrows= num_rows)\n",
    "    print(\"df shape: {}\".format(len(df)))\n",
    "    \n",
    "    #calcul aggregation - add column\n",
    "    df=difference_dates(\"order_delivered_customer_date\", \"order_approved_at\", 'time_delivered_approved_customer', df)\n",
    "    df=difference_dates(\"order_approved_at\", \"order_purchase_timestamp\", 'time_approval_purchase', df)\n",
    "    df=difference_dates(\"order_delivered_carrier_date\", \"order_approved_at\", 'time_delivery_carrier_approved', df)\n",
    "    df=difference_dates(\"order_delivered_customer_date\", \"order_delivered_carrier_date\", 'time_delivery_customer_carrier', df)\n",
    "    df=difference_dates(\"order_delivered_customer_date\", \"order_estimated_delivery_date\", 'time_delivery_customer_estimated', df)\n",
    "        \n",
    "    #df[\"order_purchase_year_month\"]=pd.to_datetime(df[\"order_purchase_timestamp\"]).dt.strftime('%Y%m')\n",
    "    df[\"order_purchase_timestamp\"]=pd.to_datetime(df[\"order_purchase_timestamp\"])\n",
    "    df=df.drop(['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date'], axis=1)\n",
    "    data, data_cat = one_hot_encoder(df, ['order_purchase_timestamp', \"order_id\"])\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return data, data_cat\n",
    "\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def order_reviews(num_rows = None, nan_as_category = True):\n",
    "    \n",
    "    ####lecture des data et encodage des variable cat\n",
    "    reviews_df = pd.read_csv('./input/olist_order_reviews_dataset.csv', nrows = num_rows)\n",
    "    orders_df, data_cat = orders(num_rows)\n",
    "    reviews= orders_df.merge(reviews_df, how='left', on='order_id')\n",
    "    #bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    print(\"reviews shape: {}\".format(len(reviews)))\n",
    "    ####calcul d'agregat pour les variables\n",
    "\n",
    "    reviews['review_comment_message_ind']=[0 if pd.isna(i.review_comment_message)==True else 1 for i in reviews.itertuples()]\n",
    "    reviews['review_comment_title_ind']=[0 if pd.isna(i.review_comment_title)==True else 1 for i in reviews.itertuples()]\n",
    "    \n",
    "    reviews['review_created_days'] = [None if pd.isna(i.review_creation_date)==True else (pd.to_datetime(i.review_creation_date) - pd.to_datetime(i.order_purchase_timestamp)).days for i in reviews.itertuples()]\n",
    "    \n",
    "    reviews['review_answer_days'] = [None if pd.isna(i.review_answer_timestamp)==True else (pd.to_datetime(i.review_answer_timestamp) - pd.to_datetime(i.review_creation_date)).days for i in reviews.itertuples()]\n",
    "    reviews_aggregations = {\n",
    "                            'review_score': ['mean'],\n",
    "                            'review_comment_message_ind': ['sum'],\n",
    "                            'review_comment_title_ind': ['sum'],\n",
    "                            'review_created_days': ['sum'],\n",
    "                            'review_answer_days': ['sum']\n",
    "                           }\n",
    "    \n",
    "    del reviews[\"review_comment_message\"]\n",
    "    del reviews[\"review_comment_title\"]\n",
    "    del reviews[\"review_creation_date\"]\n",
    "    del reviews[\"review_answer_timestamp\"]\n",
    "    reviews_agg = reviews.groupby('order_id').agg(reviews_aggregations)\n",
    "    reviews_agg.columns = pd.Index([e[0] + \"_\" + e[1].lower() for e in reviews_agg.columns.tolist()])\n",
    "    for col in reviews_agg.columns:\n",
    "        if (\"order_\" in col and \"order_id\"!=i) or \"customer_\" in col:\n",
    "            del reviews_agg[i]\n",
    "    del reviews\n",
    "    print(\"reviews agg shape: {}\".format(len(reviews_agg)))\n",
    "    gc.collect()\n",
    "    return reviews_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def order_payments(num_rows = None, nan_as_category = True):\n",
    "    payments = pd.read_csv('./input/olist_order_payments_dataset.csv', nrows = num_rows)\n",
    "    #Encode categoricals variables\n",
    "    data, data_cat = one_hot_encoder(payments)\n",
    "    #create aggregation on quantitatives variables\n",
    "    paid_aggregations = {'payment_value': ['sum'],\n",
    "                  'payment_installments': ['sum']}\n",
    "    for col in data_cat:\n",
    "        if col!=\"order_id\":\n",
    "            paid_aggregations[col] = ['sum']\n",
    "    paid_agg = data.groupby(\"order_id\").agg(paid_aggregations)\n",
    "    #rename columns\n",
    "    paid_agg.columns = pd.Index([e[0] + \"_\" + e[1].lower() for e in paid_agg.columns.tolist()])\n",
    "    \n",
    "    #aggregation quantitatives var by group\n",
    "    payment_aggregations=agg_by_group(payments, paid_agg, [\"payment_value\"], \"sum\",\"order_id\",\"payment_type\" )\n",
    "    del payments,data_cat\n",
    "    gc.collect()\n",
    "    return payment_aggregations\n",
    "\n",
    "def product_category_translation(num_rows = None, nan_as_category = True):\n",
    "    prod_transl = pd.read_csv('./input/product_category_name_translation.csv', nrows = num_rows)\n",
    "    #ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    \n",
    "    gc.collect()\n",
    "    return prod_transl\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def products(num_rows = None, nan_as_category = True):\n",
    "    products = pd.read_csv('./input/olist_products_dataset.csv', nrows = num_rows)\n",
    "    cat = product_category_translation(num_rows,nan_as_category)\n",
    "    #ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    products=products.merge(cat, how=\"left\", on=\"product_category_name\")\n",
    "    del products[\"product_category_name\"]\n",
    "    data, data_cat = one_hot_encoder(products)\n",
    "    data=data.fillna(0)\n",
    "    del cat\n",
    "    gc.collect()\n",
    "    return data, data_cat\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def sellers(num_rows = None, nan_as_category = True):\n",
    "    sellers = pd.read_csv('./input/olist_sellers_dataset.csv', nrows = num_rows)\n",
    "    #cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    \n",
    "    #del cc\n",
    "    gc.collect()\n",
    "    return sellers\n",
    "\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def order_items(num_rows = None, nan_as_category = True):\n",
    "    items = pd.read_csv('./input/olist_order_items_dataset.csv', nrows = num_rows)\n",
    "    products_obj, prods_cat=products(num_rows = None, nan_as_category = True)\n",
    "    #pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    items=items.merge(products_obj, how=\"left\", on=\"product_id\")\n",
    "    #sell=sellers(num_rows = None, nan_as_category = True)\n",
    "    #items=items.merge(sell, how=\"left\", on=\"seller_id\")\n",
    "    items_aggregations = {}\n",
    "    df_num=items.select_dtypes(include=[np.float])\n",
    "    for col in prods_cat:\n",
    "        if (\"_id\" not in col) and (\"_date\" not in col):\n",
    "              items_aggregations[col] = ['sum']\n",
    "        elif col in [\"product_id\", \"seller_id\", \"item_id\"]:\n",
    "              items_aggregations[col] = ['size']\n",
    "        elif \"_date\" in col:\n",
    "              items_aggregations[col] = ['max']\n",
    "    \n",
    "    for col_num in df_num:\n",
    "        items_aggregations[col_num] = ['mean']\n",
    "    items_agg = items.groupby(\"order_id\").agg(items_aggregations)\n",
    "    items_agg.columns = pd.Index([e[0] + \"_\" + e[1] for e in items_agg.columns.tolist()])\n",
    "    del items, products_obj\n",
    "    gc.collect()\n",
    "    \n",
    "    return items_agg\n",
    "\n",
    "\n",
    "def orders_informations(num_rows = None, nan_as_category = True):\n",
    "    with timer(\"Orders \"):\n",
    "        df, data_cat = orders(num_rows)\n",
    "        print(\"Order df shape:\", df.shape)\n",
    "        gc.collect()\n",
    "    with timer(\"Orders reviews\"):\n",
    "        reviews = order_reviews(num_rows)\n",
    "        print(\"Reviews df shape:\", reviews.shape)\n",
    "        df = df.merge(reviews, how='left', on='order_id')\n",
    "        del reviews\n",
    "        print(\"df reviews\", df.shape)\n",
    "        gc.collect()\n",
    "    with timer(\"Process order payments\"):\n",
    "        payments = order_payments(num_rows)\n",
    "        print(\"Order payments df shape:\", payments.shape)\n",
    "        df = df.merge(payments, how='left', on='order_id')\n",
    "        del payments\n",
    "        gc.collect()\n",
    "        print(\"df payment\", df.shape)\n",
    "    \n",
    "    with timer(\"Process order items\"):\n",
    "        items = order_items(num_rows)\n",
    "        print(\"Order items df shape:\", items.shape)\n",
    "        df = df.merge(items, how='left', on='order_id')\n",
    "        del items\n",
    "        gc.collect()  \n",
    "    return df\n",
    "\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def customer(num_rows = None, nan_as_category = True):\n",
    "    customer_obj = pd.read_csv('./input/olist_customers_dataset.csv', nrows = num_rows)\n",
    "    #del cc\n",
    "    gc.collect()\n",
    "    return customer_obj\n",
    "\n",
    "# Il y a des doublons dans cette table > une ville ou un code zip a différentes coordonnées\n",
    "def geolocation(num_rows = None, nan_as_category = True):\n",
    "    geoloc = pd.read_csv('./input/olist_geolocation_dataset.csv', nrows = num_rows)    \n",
    "    #del cc\n",
    "    gc.collect()\n",
    "    return geoloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3202de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: 99441\n",
      "Order df shape: (99441, 17)\n",
      "Orders  - done in 2s\n",
      "df shape: 99441\n",
      "reviews shape: 99992\n",
      "reviews agg shape: 99441\n",
      "Reviews df shape: (99441, 5)\n",
      "df reviews (99441, 22)\n",
      "Orders reviews - done in 33s\n",
      "Order payments df shape: (99440, 14)\n",
      "df payment (99441, 35)\n",
      "Process order payments - done in 1s\n",
      "Order items df shape: (98666, 82)\n",
      "Process order items - done in 2s\n",
      "orders informations shape: (99441, 117)\n",
      "Orders informations - done in 38s\n",
      "Customer df shape: (99441, 121)\n",
      "Process customer - done in 5s\n",
      "Full complete run - done in 43s\n"
     ]
    }
   ],
   "source": [
    "# lance chaque fonction avec un timer\n",
    "def main(debug = True):\n",
    "    num_rows = 1000 if debug else None\n",
    "    \n",
    "    ####import fichiers + calcul agregats (new features) et jointure de toutes les tables\n",
    "    ### + nettoyage memoire\n",
    "    with timer(\"Orders informations\"):\n",
    "        df = orders_informations(num_rows)\n",
    "        print(\"orders informations shape:\", df.shape)\n",
    "        gc.collect()\n",
    "    \n",
    "    with timer(\"Process customer\"):\n",
    "        customer_obj = customer(num_rows)\n",
    "        customer_obj = customer_obj.merge(df, how='left', on='customer_id')\n",
    "        print(\"Customer df shape:\", customer_obj.shape)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        customer_obj.columns = customer_obj.columns.str.strip()    \n",
    "        customer_obj.columns = customer_obj.columns.str.replace(' ', '_')\n",
    "        customer_obj.columns = customer_obj.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")  \n",
    "        customer_obj.columns = customer_obj.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n",
    "        \n",
    "        \n",
    "        ##############a modifier\n",
    "        ##Moyenne - median by customer\n",
    "        #count nb commande + nb produit mean + nb prod by category en moy par commande + time moyen\n",
    "        customer_aggregations={}\n",
    "        cols=list(customer_obj.columns)\n",
    "        cols_float=customer_obj.select_dtypes(include=[np.float])\n",
    "        col_ok=[]       \n",
    "        \n",
    "        \n",
    "        for col in cols:\n",
    "            if ((\"_sum\" in col) and (\"payment_\" not in col)) or ('order_status' in col) or ('order_purchase' in col) :\n",
    "                customer_aggregations[col] = ['sum']\n",
    "                col_ok.append(col)\n",
    "            if (\"payment_\" in col) or \"_size\" in col:\n",
    "                customer_aggregations[col] = ['sum', 'mean']\n",
    "                col_ok.append(col)\n",
    "            if \"order_id\" in col or \"customer_id\" in col:\n",
    "                customer_aggregations[col] = [\"size\"]\n",
    "                col_ok.append(col)\n",
    "            if (\"time_\" in col) or (\"price_\") in col or (\"review_\" in col) or (\"product\" in col and \"category\" not in col) or (\"freight\" in col):\n",
    "                customer_aggregations[col] = ['mean']\n",
    "                col_ok.append(col)\n",
    "            if 'order_purchase_timestamp' in col:\n",
    "                customer_aggregations[col] = ['max']\n",
    "                col_ok.append(col)\n",
    "        for col in cols_float:\n",
    "            if col not in col_ok :\n",
    "                customer_aggregations[col] = ['mean', 'size']\n",
    "        customer_agg = customer_obj.groupby([\"customer_unique_id\"]).agg(customer_aggregations)\n",
    "        customer_agg.columns = pd.Index([e[0] + \"_\" + e[1].lower() for e in customer_agg.columns.tolist()])\n",
    "        \n",
    "        date_max_purchase=customer_agg['order_purchase_timestamp_max'].max()\n",
    "        customer_agg=difference_dates(date_max_purchase, \"order_purchase_timestamp_max\", \"order_purchase_timestamp_recency\", customer_agg,1)\n",
    "        \n",
    "        del customer_obj\n",
    "        gc.collect()\n",
    "        return customer_agg.reset_index()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #submission_file_name = \"submission_kernel02.csv\"\n",
    "    with timer(\"Full complete run\"):\n",
    "        df = main(False)\n",
    "        #, feat_importance\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f163d6",
   "metadata": {},
   "source": [
    "# A SUPPRIMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16113c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_unique_id - count unique : 96096\n",
      "customer_id_size - count unique : 9\n",
      "order_id_size - count unique : 9\n",
      "order_purchase_timestamp_max - count unique : 95834\n",
      "time_delivered_approved_customer_mean - count unique : 251\n",
      "time_approval_purchase_mean - count unique : 37\n",
      "time_delivery_carrier_approved_mean - count unique : 128\n",
      "time_delivery_customer_carrier_mean - count unique : 248\n",
      "time_delivery_customer_estimated_mean - count unique : 331\n",
      "order_status_approved_sum - count unique : 2\n",
      "order_status_canceled_sum - count unique : 4\n",
      "order_status_created_sum - count unique : 2\n",
      "order_status_delivered_sum - count unique : 10\n",
      "order_status_invoiced_sum - count unique : 2\n",
      "order_status_processing_sum - count unique : 2\n",
      "order_status_shipped_sum - count unique : 3\n",
      "order_status_unavailable_sum - count unique : 3\n",
      "deleted unique colonne : order_status_nan_sum\n",
      "review_score_mean_mean - count unique : 34\n",
      "review_comment_message_ind_sum_mean - count unique : 17\n",
      "review_comment_title_ind_sum_mean - count unique : 12\n",
      "review_created_days_sum_mean - count unique : 226\n",
      "review_answer_days_sum_mean - count unique : 279\n",
      "payment_value_sum_sum - count unique : 28837\n",
      "payment_value_sum_mean - count unique : 29020\n",
      "payment_installments_sum_sum - count unique : 41\n",
      "payment_installments_sum_mean - count unique : 79\n",
      "payment_type_boleto_sum_sum - count unique : 6\n",
      "payment_type_boleto_sum_mean - count unique : 10\n",
      "payment_type_credit_card_sum_sum - count unique : 10\n",
      "payment_type_credit_card_sum_mean - count unique : 12\n",
      "payment_type_debit_card_sum_sum - count unique : 3\n",
      "payment_type_debit_card_sum_mean - count unique : 5\n",
      "payment_type_not_defined_sum_sum - count unique : 2\n",
      "payment_type_not_defined_sum_mean - count unique : 3\n",
      "payment_type_voucher_sum_sum - count unique : 21\n",
      "payment_type_voucher_sum_mean - count unique : 32\n",
      "deleted unique colonne : payment_type_nan_sum_sum\n",
      "deleted unique colonne : payment_type_nan_sum_mean\n",
      "boleto_payment_value_sum - count unique : 10542\n",
      "boleto_payment_value_mean - count unique : 10646\n",
      "credit_card_payment_value_sum - count unique : 26029\n",
      "credit_card_payment_value_mean - count unique : 26288\n",
      "debit_card_payment_value_sum - count unique : 1373\n",
      "debit_card_payment_value_mean - count unique : 1379\n",
      "deleted unique colonne : not_defined_payment_value_sum\n",
      "deleted unique colonne : not_defined_payment_value_mean\n",
      "voucher_payment_value_sum - count unique : 2802\n",
      "voucher_payment_value_mean - count unique : 2820\n",
      "product_id_size_mean - count unique : 39\n",
      "product_category_name_english_agro_industry_and_commerce_sum_sum - count unique : 7\n",
      "product_category_name_english_air_conditioning_sum_sum - count unique : 6\n",
      "product_category_name_english_art_sum_sum - count unique : 4\n",
      "product_category_name_english_arts_and_craftmanship_sum_sum - count unique : 3\n",
      "product_category_name_english_audio_sum_sum - count unique : 4\n",
      "product_category_name_english_auto_sum_sum - count unique : 8\n",
      "product_category_name_english_baby_sum_sum - count unique : 8\n",
      "product_category_name_english_bed_bath_table_sum_sum - count unique : 12\n",
      "product_category_name_english_books_general_interest_sum_sum - count unique : 5\n",
      "product_category_name_english_books_imported_sum_sum - count unique : 3\n",
      "product_category_name_english_books_technical_sum_sum - count unique : 4\n",
      "product_category_name_english_cds_dvds_musicals_sum_sum - count unique : 3\n",
      "product_category_name_english_christmas_supplies_sum_sum - count unique : 5\n",
      "product_category_name_english_cine_photo_sum_sum - count unique : 4\n",
      "product_category_name_english_computers_sum_sum - count unique : 5\n",
      "product_category_name_english_computers_accessories_sum_sum - count unique : 13\n",
      "product_category_name_english_consoles_games_sum_sum - count unique : 6\n",
      "product_category_name_english_construction_tools_construction_sum_sum - count unique : 9\n",
      "product_category_name_english_construction_tools_lights_sum_sum - count unique : 7\n",
      "product_category_name_english_construction_tools_safety_sum_sum - count unique : 7\n",
      "product_category_name_english_cool_stuff_sum_sum - count unique : 8\n",
      "product_category_name_english_costruction_tools_garden_sum_sum - count unique : 5\n",
      "product_category_name_english_costruction_tools_tools_sum_sum - count unique : 3\n",
      "product_category_name_english_diapers_and_hygiene_sum_sum - count unique : 5\n",
      "product_category_name_english_drinks_sum_sum - count unique : 9\n",
      "product_category_name_english_dvds_blu_ray_sum_sum - count unique : 4\n",
      "product_category_name_english_electronics_sum_sum - count unique : 7\n",
      "product_category_name_english_fashio_female_clothing_sum_sum - count unique : 5\n",
      "product_category_name_english_fashion_bags_accessories_sum_sum - count unique : 8\n",
      "product_category_name_english_fashion_childrens_clothes_sum_sum - count unique : 2\n",
      "product_category_name_english_fashion_male_clothing_sum_sum - count unique : 4\n",
      "product_category_name_english_fashion_shoes_sum_sum - count unique : 5\n",
      "product_category_name_english_fashion_sport_sum_sum - count unique : 3\n",
      "product_category_name_english_fashion_underwear_beach_sum_sum - count unique : 4\n",
      "product_category_name_english_fixed_telephony_sum_sum - count unique : 7\n",
      "product_category_name_english_flowers_sum_sum - count unique : 3\n",
      "product_category_name_english_food_sum_sum - count unique : 6\n",
      "product_category_name_english_food_drink_sum_sum - count unique : 6\n",
      "product_category_name_english_furniture_bedroom_sum_sum - count unique : 4\n",
      "product_category_name_english_furniture_decor_sum_sum - count unique : 12\n",
      "product_category_name_english_furniture_living_room_sum_sum - count unique : 8\n",
      "product_category_name_english_furniture_mattress_and_upholstery_sum_sum - count unique : 2\n",
      "product_category_name_english_garden_tools_sum_sum - count unique : 12\n",
      "product_category_name_english_health_beauty_sum_sum - count unique : 8\n",
      "product_category_name_english_home_appliances_sum_sum - count unique : 4\n",
      "product_category_name_english_home_appliances_2_sum_sum - count unique : 4\n",
      "product_category_name_english_home_comfort_2_sum_sum - count unique : 4\n",
      "product_category_name_english_home_confort_sum_sum - count unique : 5\n",
      "product_category_name_english_home_construction_sum_sum - count unique : 7\n",
      "product_category_name_english_housewares_sum_sum - count unique : 12\n",
      "product_category_name_english_industry_commerce_and_business_sum_sum - count unique : 5\n",
      "product_category_name_english_kitchen_dining_laundry_garden_furniture_sum_sum - count unique : 5\n",
      "product_category_name_english_la_cuisine_sum_sum - count unique : 3\n",
      "product_category_name_english_luggage_accessories_sum_sum - count unique : 5\n",
      "product_category_name_english_market_place_sum_sum - count unique : 6\n",
      "product_category_name_english_music_sum_sum - count unique : 2\n",
      "product_category_name_english_musical_instruments_sum_sum - count unique : 7\n",
      "product_category_name_english_office_furniture_sum_sum - count unique : 12\n",
      "product_category_name_english_party_supplies_sum_sum - count unique : 3\n",
      "product_category_name_english_perfumery_sum_sum - count unique : 7\n",
      "product_category_name_english_pet_shop_sum_sum - count unique : 8\n",
      "product_category_name_english_security_and_services_sum_sum - count unique : 2\n",
      "product_category_name_english_signaling_and_security_sum_sum - count unique : 7\n",
      "product_category_name_english_small_appliances_sum_sum - count unique : 5\n",
      "product_category_name_english_small_appliances_home_oven_and_coffee_sum_sum - count unique : 3\n",
      "product_category_name_english_sports_leisure_sum_sum - count unique : 9\n",
      "product_category_name_english_stationery_sum_sum - count unique : 7\n",
      "product_category_name_english_tablets_printing_image_sum_sum - count unique : 3\n",
      "product_category_name_english_telephony_sum_sum - count unique : 11\n",
      "product_category_name_english_toys_sum_sum - count unique : 7\n",
      "product_category_name_english_watches_gifts_sum_sum - count unique : 9\n",
      "product_category_name_english_nan_sum_sum - count unique : 7\n",
      "price_mean_mean - count unique : 8123\n",
      "freight_value_mean_mean - count unique : 8563\n",
      "product_name_lenght_mean_mean - count unique : 356\n",
      "product_description_lenght_mean_mean - count unique : 4572\n",
      "product_photos_qty_mean_mean - count unique : 107\n",
      "product_weight_g_mean_mean - count unique : 3229\n",
      "product_length_cm_mean_mean - count unique : 404\n",
      "product_height_cm_mean_mean - count unique : 373\n",
      "product_width_cm_mean_mean - count unique : 357\n",
      "order_purchase_timestamp_recency - count unique : 630\n"
     ]
    }
   ],
   "source": [
    "pk.data_uniqueone_string(df)\n",
    "for i in df.columns:\n",
    "    if \"product_\" in i:\n",
    "        df[i]=df[i].fillna(0)\n",
    "\n",
    "for i in df.columns:\n",
    "    if (\"payment_\" in i) or (\"price_\" in i) or (\"freight_\" in i)  or (\"review_\" in i):\n",
    "        df[i]=df[i].fillna(0)\n",
    "\n",
    "for i in df.columns:\n",
    "    if (\"time_\" in i):\n",
    "        df[i]=df[i].fillna(max(df[i]))\n",
    "\n",
    "del df[\"customer_id_size\"]\n",
    "\n",
    "#build 4 clusters for recency and add it to dataframe\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df[['order_purchase_timestamp_recency']])\n",
    "df['RecencyCluster'] = kmeans.predict(df.loc[:,['order_purchase_timestamp_recency']])\n",
    "\n",
    "#function for ordering cluster numbers\n",
    "def order_cluster(cluster_field_name, target_field_name,df,ascending):\n",
    "    new_cluster_field_name = 'new_' + cluster_field_name\n",
    "    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n",
    "    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n",
    "    df_new['index'] = df_new.index\n",
    "    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n",
    "    df_final = df_final.drop([cluster_field_name],axis=1)\n",
    "    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n",
    "    return df_final\n",
    "\n",
    "df_new = order_cluster('RecencyCluster', 'order_purchase_timestamp_recency',df,False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae34316c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrequencyCluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93099.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2745.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252.0</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>1.196656</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count      mean       std  min  25%  50%  75%   max\n",
       "FrequencyCluster                                                       \n",
       "0                 93099.0  1.000000  0.000000  1.0  1.0  1.0  1.0   1.0\n",
       "1                  2745.0  2.000000  0.000000  2.0  2.0  2.0  2.0   2.0\n",
       "2                   252.0  3.380952  1.196656  3.0  3.0  3.0  3.0  17.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k-means\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df_new[['order_id_size']])\n",
    "df_new['FrequencyCluster'] = kmeans.predict(df_new[['order_id_size']])\n",
    "\n",
    "#order the frequency cluster\n",
    "df_new = order_cluster('FrequencyCluster', 'order_id_size',df_new,True)\n",
    "\n",
    "#see details of each cluster\n",
    "df_new.groupby('FrequencyCluster')['order_id_size'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffc3a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RevenueCluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75389.0</td>\n",
       "      <td>94.121296</td>\n",
       "      <td>47.147486</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>86.15</td>\n",
       "      <td>129.59</td>\n",
       "      <td>200.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17189.0</td>\n",
       "      <td>306.151060</td>\n",
       "      <td>93.918776</td>\n",
       "      <td>200.29</td>\n",
       "      <td>229.24</td>\n",
       "      <td>277.86</td>\n",
       "      <td>359.62</td>\n",
       "      <td>585.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3093.0</td>\n",
       "      <td>865.050592</td>\n",
       "      <td>247.542870</td>\n",
       "      <td>585.91</td>\n",
       "      <td>664.92</td>\n",
       "      <td>788.71</td>\n",
       "      <td>1006.87</td>\n",
       "      <td>1575.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>425.0</td>\n",
       "      <td>2294.422800</td>\n",
       "      <td>1057.554535</td>\n",
       "      <td>1583.71</td>\n",
       "      <td>1760.75</td>\n",
       "      <td>2026.54</td>\n",
       "      <td>2384.50</td>\n",
       "      <td>13664.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count         mean          std      min      25%      50%  \\\n",
       "RevenueCluster                                                                 \n",
       "0               75389.0    94.121296    47.147486     0.00    55.00    86.15   \n",
       "1               17189.0   306.151060    93.918776   200.29   229.24   277.86   \n",
       "2                3093.0   865.050592   247.542870   585.91   664.92   788.71   \n",
       "3                 425.0  2294.422800  1057.554535  1583.71  1760.75  2026.54   \n",
       "\n",
       "                    75%       max  \n",
       "RevenueCluster                     \n",
       "0                129.59    200.28  \n",
       "1                359.62    585.70  \n",
       "2               1006.87   1575.05  \n",
       "3               2384.50  13664.08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(df_new[['payment_value_sum_sum']])\n",
    "df_new['RevenueCluster'] = kmeans.predict(df_new[['payment_value_sum_sum']])\n",
    "\n",
    "\n",
    "#order the cluster numbers\n",
    "df_new = order_cluster('RevenueCluster', 'payment_value_sum_sum',df_new,True)\n",
    "\n",
    "#show details of the dataframe\n",
    "df_new.groupby('RevenueCluster')['payment_value_sum_sum'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d51d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history=[]\n",
    "\n",
    "#order_purchase_timestamp_max\n",
    "def model_analyse(date1, n_days, file, var_time):\n",
    "    #entrainement m0\n",
    "    file0=file0.loc[file[var_time]<=date_1]\n",
    "    \n",
    "    #standardise\n",
    "    std_scale0 = preprocessing.StandardScaler().fit(file0)\n",
    "    file0_std0 = std_scale0.transform(file0)\n",
    "    \n",
    "    #entrainement du m0\n",
    "    kmeans_m0 = KMeans(n_clusters=4)\n",
    "    C0 = kmeans_mo.fit(file0_std0)\n",
    "    \n",
    "    #file 1\n",
    "    end_date = pd.to_datetime(date_1) + datetime.timedelta(days=n_days)\n",
    "    file1=file1.loc[(file[var_time]>date1) & (file[var_time]<end_date)]\n",
    "    \n",
    "    #entrainement m1\n",
    "    kmeans_m1 = KMeans(n_clusters=4)\n",
    "    std_scale1 = preprocessing.StandardScaler().fit(file1)\n",
    "    file1_std1 = std_scale1.transform(file1)\n",
    "    C1_new = kmeans_m1.fit(file1_std1)\n",
    "    \n",
    "    #predict file 1 avec m0\n",
    "    file1_std0 = std_scale0.transform(file1)\n",
    "    C1_init = kmeans_m0.predict(file1_std0)\n",
    "    \n",
    "    #calcul ARI\n",
    "    ARI=adjusted_rand_score(C1_init, C1_new)\n",
    "    return ARI\n",
    "\n",
    "def find_best_periode_update_model(date1, n_days, file, var_time):\n",
    "    num_model=1\n",
    "    date_end=max(file[var_time])\n",
    "    date1=pd.to_datetime(date1)\n",
    "    while date1<=date_end:\n",
    "        ARI = model_analyse(date1, n_days, file, var_time)\n",
    "        \n",
    "        num_model=num_model+1\n",
    "        date1 = pd.to_datetime(date_1) + datetime.timedelta(days=n_days)\n",
    "        metrics_history.append({'model':num_model, 'n_days':n_days, 'date1':date1, 'ARI':ARI})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588efb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-10-17 17:30:18')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df_new[\"order_purchase_timestamp_max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46c914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
