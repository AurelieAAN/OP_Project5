{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b2d052",
   "metadata": {},
   "source": [
    "# Création d'une segmentation client pour Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e147f4",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daef037",
   "metadata": {},
   "source": [
    "Problématique : identifier différents comportements des clients afin de pouvoir faire du ciblage marketing et augmenter ainsi la rentabilité de l'entreprise\n",
    "\n",
    "Nous disposons de plusieurs informations anonymisées de la société Olist sur : \n",
    "- les commandes\n",
    "- les clients\n",
    "- les produits\n",
    "- les catégories des produits\n",
    "- des informations sur le paiment des commandes\n",
    "- les vendeurs.\n",
    "\n",
    "Il est important de noter qu'une commande peut contenir des produits de différents vendeurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97df084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notre package de fonctionnalités\n",
    "from Package import Scripts_Analyse01 as pk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from yellowbrick.features import ParallelCoordinates\n",
    "from plotly.graph_objects import Layout\n",
    "import jenkspy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n",
    "from sklearn import preprocessing\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.features import PCA as PCA_yellow\n",
    "from yellowbrick.style import set_palette\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "#Import knearest neighbors Classifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,mutual_info_classif\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    " \n",
    "import mlflow\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073e7c9",
   "metadata": {},
   "source": [
    "Regardons les différentes tables à notre disposition et s'il y a des doublons où des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27599a47",
   "metadata": {},
   "source": [
    "Commençons par la table des commandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('./input/olist_orders_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a7286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace2b15",
   "metadata": {},
   "source": [
    "Nous avons 99 441 lignes et 8 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47677b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders[['order_id']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d5936",
   "metadata": {},
   "source": [
    "Nous avons une ligne par commande et il n'y a pas de doublons dans cette table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12711f",
   "metadata": {},
   "source": [
    "Regardons si cette table contient des données manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(orders, 0.0000000001,0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9da9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b861988",
   "metadata": {},
   "source": [
    "Cette table contient très peu de données manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2caa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pk.matrix_vm(orders, (16,8), (0.60, 0.64, 0.49))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124af70e",
   "metadata": {},
   "source": [
    "Nous observons des données manquantes pour les dates de livraison par exemple. Nous pouvons conserver ces données car ce sont surement des commandes non livrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c8971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders.loc[pd.isna(orders[\"order_approved_at\"])==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8cf875",
   "metadata": {},
   "source": [
    "Nous avons des données manquantes pour les commandes annulées, ce qui est normal car nous n'avons pas les informations.\n",
    "Est ce qu'il est intéressant de conserver ces données ??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db06cab",
   "metadata": {},
   "source": [
    "Il serait interessant de créer des variables comme le délai de livraison au client, délai de livraison au transporteur et le délai entre la livraison transporteur et la livraison client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c207601",
   "metadata": {},
   "source": [
    "Nous créerons une fonction pour la création des variables et l'aggrégation ensuite. Passons à l'étude de la table reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('./input/olist_order_reviews_dataset.csv', nrows = num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e65c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[['review_id', 'order_id']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88babb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(reviews, 0.0000000001,0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c88461",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83625522",
   "metadata": {},
   "source": [
    "Il n'y a pas de doublons dans cette table. Les colonnes review_comment_title et review_comment_message\tcontiennent des données vides, mais le client n'est pas obligé de remplir ces données. Nous créerons des colonnes binaires sur ces colonnes et nous ferons des aggrégations par commande par la suite.\n",
    "Etudions la table concernant les paiements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debd3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "payments = pd.read_csv('./input/olist_order_payments_dataset.csv', nrows = num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1338ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "payments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90764839",
   "metadata": {},
   "outputs": [],
   "source": [
    "payments[['order_id', 'payment_sequential']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdffc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(payments, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f9619",
   "metadata": {},
   "source": [
    "Il n'y a pas de doublons et il n'y a pas de données manquantes dans cette table. Nous allons calculer les sommes par commandes et les moyennes. Nous créerons une fonction pour faire ce traitement. Analysons la table des produits, des catégories de produits et des vendeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767681ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_transl = pd.read_csv('./input/product_category_name_translation.csv', nrows = num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc731b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('./input/olist_products_dataset.csv', nrows = num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = pd.read_csv('./input/olist_sellers_dataset.csv', nrows = num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_transl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_transl[['product_category_name']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(prod_transl, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365795f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37106ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "products[['product_id']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5e3ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(products, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386ed8a",
   "metadata": {},
   "source": [
    "Nous pouvons remplacer les vides par 0 pour la table produit, car cela nous indiquera qu'il n'y a pas d'information pour ces produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408b49e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sellers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers[['seller_id']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(sellers, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ef332",
   "metadata": {},
   "source": [
    "Il n'y a pas de doublons dans ces tables. Il existe des données manquantes dans la tables produits, mais très peu. Nous conservons les données telles quelles. Regardons la table contenant les produits par commande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('./input/olist_order_items_dataset.csv', nrows = num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5f036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633dd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[['order_id','product_id', 'order_item_id']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0cde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(items, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebdf03",
   "metadata": {},
   "source": [
    "Il n'y a aucun doublons et aucunes données manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53c51a",
   "metadata": {},
   "source": [
    "Regardons la table clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_obj = pd.read_csv('./input/olist_customers_dataset.csv', nrows = num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4584b",
   "metadata": {},
   "source": [
    "Nous observons qu'un customer_unique_id peut avoir plusieurs customer_id. Un client peut donc avoir plusieurs comptes client. Il faudra réaliser une aggrégation grâce à l'id unique du client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eab232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_obj[['customer_id','customer_unique_id']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(customer_obj, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb9ac3",
   "metadata": {},
   "source": [
    "Il n'y a pas de doublons et il n'y a pas de données manquantes dans cette table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d37f4",
   "metadata": {},
   "source": [
    "Regardons la table de geolocalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ec697",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc = pd.read_csv('./input/olist_geolocation_dataset.csv', nrows = num_rows)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ff6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc[['geolocation_zip_code_prefix','geolocation_city','geolocation_state']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ef4cf",
   "metadata": {},
   "source": [
    "Nous avons beaucoup de doublons dans cette table. Nous n'allons pas l'utiliser.\n",
    "Nous pourrions par la suite rechercher une table des localisations des villes du Brésil sur internet afin d'ajouter une localisation des clients par ville.\n",
    "\n",
    "Nous pouvons à présent fusionner nos tables et faire des aggrégations par client unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac185ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "#####encoder les colonnes catégorielles\n",
    "def one_hot_encoder(df, exclude=[], nan_as_category = True):\n",
    "    cols=filter(lambda x: (\"_id\" not in (x)) and x not in (exclude), df.columns)\n",
    "    original_columns = list(cols)\n",
    "    categorical_columns = [col for col in original_columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "def agg_by_group(df_test2, tab, cols, agg_name, col_index, col_by_cat):\n",
    "    for col in cols:\n",
    "        df_calcul=df_test2.pivot_table(values=col, index=col_index, columns=col_by_cat, aggfunc=agg_name).reset_index()\n",
    "        lt_col=list(filter(lambda x: x not in (col_index), df_calcul.columns))\n",
    "        lt_col=[e + \"_\"+str(col) for e in lt_col]\n",
    "        lt_col.insert(0,col_index)\n",
    "        df_calcul.columns=pd.Index([e for e in lt_col])\n",
    "        df_calcul=df_calcul.fillna(0)\n",
    "        tab=tab.merge(df_calcul, how='left', on=col_index)\n",
    "        #display(tab)\n",
    "    return tab\n",
    "\n",
    "def difference_dates(date2, date1, new_column, df, var_i=0):    \n",
    "    df2=df.copy()\n",
    "    if var_i==0:\n",
    "        df2[new_column] = (pd.to_datetime(df2[date2]) - pd.to_datetime(df2[date1])).dt.days\n",
    "    if var_i==1:\n",
    "        df2[new_column] = (pd.to_datetime(date2) - pd.to_datetime(df2[date1])).dt.days\n",
    "    return df2\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def orders(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    ####lecture des fichiers application_train et application test\n",
    "    df = pd.read_csv('./input/olist_orders_dataset.csv', nrows= num_rows)\n",
    "    print(\"df shape: {}\".format(len(df)))\n",
    "    \n",
    "    #calcul aggregation - add column\n",
    "    df=difference_dates(\"order_delivered_customer_date\", \"order_approved_at\", 'time_delivered_approved_customer', df)\n",
    "    df=difference_dates(\"order_approved_at\", \"order_purchase_timestamp\", 'time_approval_purchase', df)\n",
    "    df=difference_dates(\"order_delivered_carrier_date\", \"order_approved_at\", 'time_delivery_carrier_approved', df)\n",
    "    df=difference_dates(\"order_delivered_customer_date\", \"order_delivered_carrier_date\", 'time_delivery_customer_carrier', df)\n",
    "    df=difference_dates(\"order_delivered_customer_date\", \"order_estimated_delivery_date\", 'time_delivery_customer_estimated', df)\n",
    "        \n",
    "    df[\"order_purchase_year_month\"]=pd.to_datetime(df[\"order_purchase_timestamp\"]).dt.strftime('%Y%m')\n",
    "    df[\"order_purchase_timestamp\"]=pd.to_datetime(df[\"order_purchase_timestamp\"])\n",
    "    df=df.drop(['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date'], axis=1)\n",
    "    data, data_cat = one_hot_encoder(df, ['order_purchase_timestamp', \"order_id\"])\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return data, data_cat\n",
    "\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def order_reviews(num_rows = None, nan_as_category = True):\n",
    "    date1 = datetime.now()\n",
    "    ####lecture des data et encodage des variable cat\n",
    "    reviews = pd.read_csv('./input/olist_order_reviews_dataset.csv', nrows = num_rows)\n",
    "    #bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    print(\"reviews shape: {}\".format(len(reviews)))\n",
    "    ####calcul d'agregat pour les variables\n",
    "\n",
    "    reviews['review_comment_message_ind']=[0 if pd.isna(i.review_comment_message)==True else 1 for i in reviews.itertuples()]\n",
    "    reviews['review_comment_title_ind']=[0 if pd.isna(i.review_comment_title)==True else 1 for i in reviews.itertuples()]\n",
    "    reviews['review_created_days'] = [None if pd.isna(i.review_creation_date)==True else (pd.to_datetime(i.review_creation_date) - date1).days for i in reviews.itertuples()]\n",
    "    reviews['review_answer_days'] = [None if pd.isna(i.review_answer_timestamp)==True else (pd.to_datetime(i.review_answer_timestamp) - date1).days for i in reviews.itertuples()]\n",
    "    reviews_aggregations = {\n",
    "                            'review_score': ['mean'],\n",
    "                            'review_comment_message_ind': ['sum'],\n",
    "                            'review_comment_title_ind': ['sum'],\n",
    "                            'review_created_days': ['sum'],\n",
    "                            'review_answer_days': ['sum']\n",
    "                           }\n",
    "    \n",
    "    del reviews[\"review_comment_message\"]\n",
    "    del reviews[\"review_comment_title\"]\n",
    "    del reviews[\"review_creation_date\"]\n",
    "    del reviews[\"review_answer_timestamp\"]\n",
    "    reviews_agg = reviews.groupby('order_id').agg(reviews_aggregations)\n",
    "    reviews_agg.columns = pd.Index([e[0] + \"_\" + e[1].lower() for e in reviews_agg.columns.tolist()])\n",
    "    del reviews\n",
    "    print(\"reviews agg shape: {}\".format(len(reviews_agg)))\n",
    "    gc.collect()\n",
    "    return reviews_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def order_payments(num_rows = None, nan_as_category = True):\n",
    "    payments = pd.read_csv('./input/olist_order_payments_dataset.csv', nrows = num_rows)\n",
    "    #Encode categoricals variables\n",
    "    data, data_cat = one_hot_encoder(payments)\n",
    "    #create aggregation on quantitatives variables\n",
    "    paid_aggregations = {'payment_value': ['sum'],\n",
    "                  'payment_installments': ['sum']}\n",
    "    for col in data_cat:\n",
    "        if col!=\"order_id\":\n",
    "            paid_aggregations[col] = ['sum']\n",
    "    paid_agg = data.groupby(\"order_id\").agg(paid_aggregations)\n",
    "    #rename columns\n",
    "    paid_agg.columns = pd.Index([e[0] + \"_\" + e[1].lower() for e in paid_agg.columns.tolist()])\n",
    "    \n",
    "    #aggregation quantitatives var by group\n",
    "    payment_aggregations=agg_by_group(payments, paid_agg, [\"payment_value\"], \"sum\",\"order_id\",\"payment_type\" )\n",
    "    del payments,data_cat\n",
    "    gc.collect()\n",
    "    return payment_aggregations\n",
    "\n",
    "def product_category_translation(num_rows = None, nan_as_category = True):\n",
    "    prod_transl = pd.read_csv('./input/product_category_name_translation.csv', nrows = num_rows)\n",
    "    #ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    \n",
    "    gc.collect()\n",
    "    return prod_transl\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def products(num_rows = None, nan_as_category = True):\n",
    "    products = pd.read_csv('./input/olist_products_dataset.csv', nrows = num_rows)\n",
    "    cat = product_category_translation(num_rows,nan_as_category)\n",
    "    #ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    products=products.merge(cat, how=\"left\", on=\"product_category_name\")\n",
    "    del products[\"product_category_name\"]\n",
    "    data, data_cat = one_hot_encoder(products)\n",
    "    data=data.fillna(0)\n",
    "    del cat\n",
    "    gc.collect()\n",
    "    return data, data_cat\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def sellers(num_rows = None, nan_as_category = True):\n",
    "    sellers = pd.read_csv('./input/olist_sellers_dataset.csv', nrows = num_rows)\n",
    "    #cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    \n",
    "    #del cc\n",
    "    gc.collect()\n",
    "    return sellers\n",
    "\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def order_items(num_rows = None, nan_as_category = True):\n",
    "    items = pd.read_csv('./input/olist_order_items_dataset.csv', nrows = num_rows)\n",
    "    products_obj, prods_cat=products(num_rows = None, nan_as_category = True)\n",
    "    #pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    items=items.merge(products_obj, how=\"left\", on=\"product_id\")\n",
    "    #sell=sellers(num_rows = None, nan_as_category = True)\n",
    "    #items=items.merge(sell, how=\"left\", on=\"seller_id\")\n",
    "    items_aggregations = {}\n",
    "    df_num=items.select_dtypes(include=[np.float])\n",
    "    for col in prods_cat:\n",
    "        if (\"_id\" not in col) and (\"_date\" not in col):\n",
    "              items_aggregations[col] = ['sum']\n",
    "        elif col in [\"product_id\", \"seller_id\", \"item_id\"]:\n",
    "              items_aggregations[col] = ['size']\n",
    "        elif \"_date\" in col:\n",
    "              items_aggregations[col] = ['max']\n",
    "    \n",
    "    for col_num in df_num:\n",
    "        items_aggregations[col_num] = ['mean']\n",
    "    items_agg = items.groupby(\"order_id\").agg(items_aggregations)\n",
    "    items_agg.columns = pd.Index([e[0] + \"_\" + e[1] for e in items_agg.columns.tolist()])\n",
    "    del items, products_obj\n",
    "    gc.collect()\n",
    "    \n",
    "    return items_agg\n",
    "\n",
    "\n",
    "def orders_informations(num_rows = None, nan_as_category = True):\n",
    "    with timer(\"Orders \"):\n",
    "        df, data_cat = orders(num_rows)\n",
    "        print(\"Order df shape:\", df.shape)\n",
    "        gc.collect()\n",
    "    with timer(\"Orders reviews\"):\n",
    "        reviews = order_reviews(num_rows)\n",
    "        print(\"Reviews df shape:\", reviews.shape)\n",
    "        df = df.merge(reviews, how='left', on='order_id')\n",
    "        del reviews\n",
    "        gc.collect()\n",
    "    with timer(\"Process order payments\"):\n",
    "        payments = order_payments(num_rows)\n",
    "        print(\"Order payments df shape:\", payments.shape)\n",
    "        df = df.merge(payments, how='left', on='order_id')\n",
    "        del payments\n",
    "        gc.collect()\n",
    "    \n",
    "    with timer(\"Process order items\"):\n",
    "        items = order_items(num_rows)\n",
    "        print(\"Order items df shape:\", items.shape)\n",
    "        df = df.merge(items, how='left', on='order_id')\n",
    "        del items\n",
    "        gc.collect()  \n",
    "    return df\n",
    "\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def customer(num_rows = None, nan_as_category = True):\n",
    "    customer_obj = pd.read_csv('./input/olist_customers_dataset.csv', nrows = num_rows)\n",
    "    #del cc\n",
    "    gc.collect()\n",
    "    return customer_obj\n",
    "\n",
    "# Il y a des doublons dans cette table > une ville ou un code zip a différentes coordonnées\n",
    "def geolocation(num_rows = None, nan_as_category = True):\n",
    "    geoloc = pd.read_csv('./input/olist_geolocation_dataset.csv', nrows = num_rows)    \n",
    "    #del cc\n",
    "    gc.collect()\n",
    "    return geoloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3202de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lance chaque fonction avec un timer\n",
    "def main(debug = True):\n",
    "    num_rows = 1000 if debug else None\n",
    "    \n",
    "    ####import fichiers + calcul agregats (new features) et jointure de toutes les tables\n",
    "    ### + nettoyage memoire\n",
    "    with timer(\"Orders informations\"):\n",
    "        df = orders_informations(num_rows)\n",
    "        print(\"orders informations shape:\", df.shape)\n",
    "        gc.collect()\n",
    "    \n",
    "    with timer(\"Process customer\"):\n",
    "        customer_obj = customer(num_rows)\n",
    "        customer_obj = customer_obj.merge(df, how='left', on='customer_id')\n",
    "        print(\"Customer df shape:\", customer_obj.shape)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        customer_obj.columns = customer_obj.columns.str.strip()    \n",
    "        customer_obj.columns = customer_obj.columns.str.replace(' ', '_')\n",
    "        customer_obj.columns = customer_obj.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")  \n",
    "        customer_obj.columns = customer_obj.columns.str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n",
    "        \n",
    "        \n",
    "        ##############a modifier\n",
    "        ##Moyenne - median by customer\n",
    "        #count nb commande + nb produit mean + nb prod by category en moy par commande + time moyen\n",
    "        customer_aggregations={}\n",
    "        cols=list(customer_obj.columns)\n",
    "        cols_float=customer_obj.select_dtypes(include=[np.float])\n",
    "        col_ok=[]       \n",
    "        \n",
    "        \n",
    "        for col in cols:\n",
    "            if ((\"_sum\" in col) and (\"payment_\" not in col)) or ('order_status' in col) or ('order_purchase' in col) :\n",
    "                customer_aggregations[col] = ['sum']\n",
    "                col_ok.append(col)\n",
    "            if (\"payment_\" in col) or \"_size\" in col:\n",
    "                customer_aggregations[col] = ['sum', 'mean']\n",
    "                col_ok.append(col)\n",
    "            if \"order_id\" in col or \"customer_id\" in col:\n",
    "                customer_aggregations[col] = [\"size\"]\n",
    "                col_ok.append(col)\n",
    "            if \"time_\" in col:\n",
    "                customer_aggregations[col] = ['mean']\n",
    "                col_ok.append(col)\n",
    "            if 'order_purchase_timestamp' in col:\n",
    "                customer_aggregations[col] = ['max']\n",
    "                col_ok.append(col)\n",
    "        for col in cols_float:\n",
    "            if col not in col_ok :\n",
    "                customer_aggregations[col] = ['mean', 'size']\n",
    "        customer_agg = customer_obj.groupby([\"customer_unique_id\"]).agg(customer_aggregations)\n",
    "        customer_agg.columns = pd.Index([e[0] + \"_\" + e[1].lower() for e in customer_agg.columns.tolist()])\n",
    "        \n",
    "        date_max_purchase=customer_agg['order_purchase_timestamp_max'].max()\n",
    "        customer_agg=difference_dates(date_max_purchase, \"order_purchase_timestamp_max\", \"order_purchase_timestamp_recency\", customer_agg,1)\n",
    "        \n",
    "        del customer_obj\n",
    "        gc.collect()\n",
    "        return customer_agg.reset_index()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #submission_file_name = \"submission_kernel02.csv\"\n",
    "    with timer(\"Full complete run\"):\n",
    "        df = main(False)\n",
    "        #, feat_importance\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736615ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8312b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['customer_unique_id']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdc544",
   "metadata": {},
   "source": [
    "Nous avons à présent une ligne par client, au total 96 096 et 169 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97128b",
   "metadata": {},
   "source": [
    "Supprimons les colonnes avec des valeurs uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.data_uniqueone_string(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903b9f4",
   "metadata": {},
   "source": [
    "Il reste 163 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc6342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ceb82",
   "metadata": {},
   "source": [
    "Nous avions quelques données manquantes. Regardons les de plus près."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6185bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.matrix_vm(df, (16,8), (0.60, 0.64, 0.49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a446a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(df, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f4658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[pd.isna(df[\"product_id_size_mean\"])==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6727c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    if \"product_\" in i:\n",
    "        df[i]=df[i].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210c706",
   "metadata": {},
   "source": [
    "Regardons les autres colonnes, il y a des valeurs manquantes car la commande n'a pas été approuvée. Remplaçons toutes les valeurs par 0 pour les variables : payment price, freight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d2890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(df, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d16314",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    if (\"payment_\" in i) or (\"price_\" in i) or (\"freight_\" in i)  or (\"review_\" in i):\n",
    "        df[i]=df[i].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e920e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(df, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c394db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_delivery_customer_estimated_mean\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53012a76",
   "metadata": {},
   "source": [
    "Nous décidons de mettre 365 pour les variables time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    if (\"time_\" in i):\n",
    "        df[i]=df[i].fillna(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d595c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=pk.del_Nan(df, 0.0000000001,0, 0)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fa326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59612fcf",
   "metadata": {},
   "source": [
    "## Analyse exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab590d3a",
   "metadata": {},
   "source": [
    "Les colonnes customer_id_size et order_id_size sont identiques. Nous supprimons la colonne customer_id_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"customer_id_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485dbc7d",
   "metadata": {},
   "source": [
    "Il ne semble pas qu'il y ait des données aberrantes.\n",
    "Cependant, nous observons des variables avec des données uniques. Supprimons ces variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94069c",
   "metadata": {},
   "source": [
    "Il reste 163 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b600821",
   "metadata": {},
   "source": [
    "Les données sont peu dispersées au sein des variables, nous observons que toutes les boites à moustache sont aplaties.\n",
    "Nous observons quand même des valeurs assez hautes. Il serait intéressant de créer des regoupements ordinaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f2bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e49eefaa",
   "metadata": {},
   "source": [
    "Realisons un test de skewness pour voir comment se comporte les distributions.\n",
    "\n",
    "Nous rappelons :\n",
    "- Si y1=0 alors la distribution est symétrique.\n",
    "- Si y1>0 alors la distribution est étalée à droite.\n",
    "- Si y1<0 alors la distribution est étalée à gauche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df_i=df[col].loc[pd.isna(df[col])==False]\n",
    "    print(\"Variable : \"+col+\" ----- y1=\"+ str(df[col].skew()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c1467",
   "metadata": {},
   "source": [
    "La plupart des distributions des variables sont étalées à droite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcc07b",
   "metadata": {},
   "source": [
    "Regardons comment nos variables se comportent par rapport à la loi normale grâce au test de kurtosis.\n",
    "\n",
    "Nous rappelons les informations suivantes :\n",
    "- Si γ2=0 , alors la distribution a le même aplatissement que la distribution normale.\n",
    "- Si γ2>0 , alors elle est moins aplatie que la distribution normale : les observations sont plus concentrées.\n",
    "- Si γ2<0 , alors les observations sont moins concentrées : la distribution est plus aplatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df_i=df[col].loc[pd.isna(df[col])==False]\n",
    "    print(\"Variable : \"+col+\" -- y2=\"+ str(df[col].kurtosis()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c641ee4",
   "metadata": {},
   "source": [
    "La majorité des distributions des variables sont moins aplaties que la distribution de la loi normale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd850c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].nunique()<20 and df[col].nunique()>=6:\n",
    "        pk.graph_barplot(df[col], \"Répartition des clients selon la variable\"+col, \n",
    "              (0.82, 0.28, 0.09),\n",
    "              0, 40, col, \"Fréquence en %\",0,1, (11,7))\n",
    "    if df[col].nunique()<6 and df[col].nunique()>2:\n",
    "        pk.graph_circle(df[col], col, \"Répartition des clients en fonction de la variable \"+col)\n",
    "    if df[col].nunique()<=2:\n",
    "        pk.graph_pie(df[col], col, \"Répartition des clients selon la variable \"+col,[\"#f56315\", \"#f88e55\"], (15,7)),\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed7649",
   "metadata": {},
   "source": [
    "###############commentaires!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191e028",
   "metadata": {},
   "source": [
    "Il serait interessant de creer des regroupements ordinaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_fisher_jenks(data, col, new_col, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c1c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f867cdbc",
   "metadata": {},
   "source": [
    "## Analyse bivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7022fea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b150a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8aaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40099498",
   "metadata": {},
   "source": [
    "## correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3b9d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,4)})\n",
    "\n",
    "data_corr = df.corr()\n",
    "\n",
    "display(data_corr)\n",
    "ax = sns.heatmap(data_corr, xticklabels = data_corr.columns , \n",
    "                 yticklabels = data_corr.columns, cmap = 'rocket_r')\n",
    "plt.title(\"Matrice de corrélation\")\n",
    "\n",
    "plt.xlabel(\"Variables\")\n",
    "\n",
    "plt.ylabel(\"Variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ce265",
   "metadata": {},
   "source": [
    "## ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f71683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c147df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acp=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1160b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=pk.amulti_acp_standard(df_acp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d07bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94e5948a",
   "metadata": {},
   "source": [
    "## Zoom segmentation RFM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8e1f8",
   "metadata": {},
   "source": [
    "### Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"order_purchase_timestamp_recency\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.graph_hist(df[\"order_purchase_timestamp_recency\"],[0,50, 100, 200,300,400,500,600,772] ,\"Distribution des clients en fonction de la variable order_purchase_timestamp_recency\",\n",
    "              \"#f88e55\", 0,772, 100, 0, 35000, \"order_purchase_timestamp_recency\", 'Fréquences',(11,7), 0, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef1fd9",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdbb113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea1f4ab",
   "metadata": {},
   "source": [
    "# ########mettre en %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"order_id_size\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.graph_hist(df[\"order_id_size\"],[0,1,2,3,4,5,6,7,8,9,10,15,17] ,\"Distribution des clients en fonction de la variable order_id_size\",\n",
    "              \"#f88e55\", 0,17, 1, 0, 96000, \"order_id_size\", 'Fréquences',(11,7), 0, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef11d4",
   "metadata": {},
   "source": [
    "### Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d799848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"payment_value_sum_sum\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.graph_hist(df[\"payment_value_sum_sum\"],[0,10,20,30,40,50,60,70,80,90, 100,500,1000,2000,13664.08] ,\"Distribution des clients en fonction de la variable payment_value_sum_sum\",\n",
    "              \"#f88e55\", 0,13700, 10, 0, 55000, \"payment_value_sum_sum\", 'Fréquences',(11,7), 0, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2d24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
